#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç ”ç©¶ä»£ç† - AutoGen v0.4+ APIç‰ˆæœ¬
===============================

åŸºäºAutoGen v0.4+çš„ç°ä»£åŒ–ç ”ç©¶ä»£ç†å®ç°ï¼Œ
é›†æˆBrave Search APIå’Œè®¤çŸ¥å¢å¼ºåŠŸèƒ½ã€‚
"""

import os
import asyncio
import logging
import json
import aiohttp
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

from autogen_agentchat.messages import ChatMessage, TextMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient

# å¯¼å…¥è‡ªå®šä¹‰ Gemini å®¢æˆ·ç«¯
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from clients.gemini_client import create_gemini_client

from dotenv import load_dotenv

# å¯¼å…¥åŸºç¡€ä»£ç†
from agents.base_agent_v4 import EnhancedAssistantAgent
from cognitive_context.cognitive_analysis import CognitiveTools, CognitiveLevel

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class EnhancedResearchAgent(EnhancedAssistantAgent):
    """å¢å¼ºå‹ç ”ç©¶ä»£ç† (v0.4)"""
    
    def __init__(
        self,
        name: str = "ç ”ç©¶ä»£ç†",
        model_client: Optional[OpenAIChatCompletionClient] = None,
        brave_api_key: Optional[str] = None,
        search_engine_url: Optional[str] = None,
        search_engine_api_key: Optional[str] = None,
        cognitive_level: CognitiveLevel = CognitiveLevel.DEEP,
        **kwargs
    ):
        """
        åˆå§‹åŒ–å¢å¼ºå‹ç ”ç©¶ä»£ç†ã€‚
        
        Args:
            name: ä»£ç†åç§°
            model_client: OpenAIæ¨¡å‹å®¢æˆ·ç«¯
            brave_api_key: Brave Search APIå¯†é’¥
            cognitive_level: è®¤çŸ¥æ°´å¹³
        """
        # æ„å»ºç ”ç©¶ä»£ç†çš„ç³»ç»Ÿæ¶ˆæ¯
        system_message = self._build_research_system_message()
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(
            name=name,
            model_client=model_client,
            agent_type="research",
            cognitive_level=cognitive_level,
            system_message=system_message,
            **kwargs
        )
        
        # ç ”ç©¶ä»£ç†ç‰¹æœ‰é…ç½®
        self.brave_api_key = brave_api_key or os.getenv("BRAVE_API_KEY")
        self.search_engine_url = search_engine_url or os.getenv("SEARCH_ENGINE_BASE_URL")
        self.search_engine_api_key = search_engine_api_key or os.getenv("SEARCH_ENGINE_API_KEY")
        
        # é€‰æ‹©æœç´¢å¼•æ“
        if self.search_engine_url:
            self.search_endpoint = self.search_engine_url
            self.use_searxng = True
        else:
            self.search_endpoint = "https://api.search.brave.com/res/v1/web/search"
            self.use_searxng = False
        
        # ç ”ç©¶çŠ¶æ€
        self.research_history = []
        self.search_cache = {}
        self.research_metrics = {
            "searches_performed": 0,
            "sources_analyzed": 0,
            "reports_generated": 0
        }
        
        logger.info(f"å¢å¼ºå‹ç ”ç©¶ä»£ç† '{name}' åˆå§‹åŒ–å®Œæˆ (v0.4 API)")

    def _build_research_system_message(self) -> str:
        """æ„å»ºç ”ç©¶ä»£ç†çš„ç³»ç»Ÿæ¶ˆæ¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIç ”ç©¶ä»£ç†ï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

ğŸ” **ç ”ç©¶èƒ½åŠ›**:
- ä½¿ç”¨Brave Search APIè¿›è¡Œæ·±åº¦ç½‘ç»œæœç´¢
- å¤šæºä¿¡æ¯æ”¶é›†å’Œäº¤å‰éªŒè¯
- ç»“æ„åŒ–ä¿¡æ¯åˆ†æå’Œæ•´ç†

ğŸ§  **è®¤çŸ¥å¢å¼º**:
- æ·±åº¦è®¤çŸ¥åˆ†æå’Œæ¨ç†
- Context Engineeringç†è®ºåº”ç”¨
- æ‰¹åˆ¤æ€§æ€ç»´å’Œä¿¡æ¯è¯„ä¼°

ğŸ“Š **æŠ¥å‘Šç”Ÿæˆ**:
- ä¸“ä¸šç ”ç©¶æŠ¥å‘Šæ’°å†™
- æ•°æ®å¯è§†åŒ–å»ºè®®
- å¼•ç”¨å’Œæ¥æºæ ‡æ³¨

è¯·åœ¨ç ”ç©¶è¿‡ç¨‹ä¸­ï¼š
1. ä½¿ç”¨å¤šä¸ªå¯é æ¥æº
2. è¿›è¡Œæ‰¹åˆ¤æ€§åˆ†æ
3. æä¾›ç»“æ„åŒ–çš„ç ”ç©¶ç»“æœ
4. æ ‡æ³¨ä¿¡æ¯æ¥æºå’Œå¯ä¿¡åº¦"""

    async def on_messages(self, messages: List[ChatMessage], cancellation_token) -> ChatMessage:
        """å¤„ç†æ¶ˆæ¯å¹¶æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        try:
            # è·å–æœ€æ–°æ¶ˆæ¯
            if not messages:
                return TextMessage(
                    content="è¯·æä¾›éœ€è¦ç ”ç©¶çš„ä¸»é¢˜æˆ–é—®é¢˜ã€‚",
                    source=self.name
                )
            
            last_message = messages[-1]
            query = last_message.content if hasattr(last_message, 'content') else str(last_message)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç ”ç©¶è¯·æ±‚
            if self._is_research_request(query):
                # æ‰§è¡Œç ”ç©¶ä»»åŠ¡
                research_result = await self._perform_research(query)
                
                # ç”Ÿæˆç ”ç©¶æŠ¥å‘Š
                report = await self._generate_research_report(query, research_result)
                
                return TextMessage(
                    content=report,
                    source=self.name
                )
            else:
                # ä½¿ç”¨çˆ¶ç±»å¤„ç†æ™®é€šå¯¹è¯
                return await super().on_messages(messages, cancellation_token)
                
        except Exception as e:
            logger.error(f"ç ”ç©¶ä»£ç†æ¶ˆæ¯å¤„ç†é”™è¯¯: {e}")
            return TextMessage(
                content=f"ç ”ç©¶è¿‡ç¨‹ä¸­é‡åˆ°é”™è¯¯: {str(e)}",
                source=self.name
            )

    def _is_research_request(self, query: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç ”ç©¶è¯·æ±‚"""
        # éœ€è¦å®æ—¶ä¿¡æ¯çš„å…³é”®è¯ï¼ˆåº”è¯¥ä½¿ç”¨æœç´¢ï¼‰
        realtime_keywords = [
            "ä»Šå¤©", "ç°åœ¨", "å½“å‰", "æœ€æ–°", "å®æ—¶", "æ—¥æœŸ", "æ—¶é—´", "æ–°é—»", "è¿›å±•", "èµ„è®¯",
            "today", "now", "current", "latest", "real-time", "date", "time", "news", "update"
        ]
        
        # ä¼ ç»Ÿç ”ç©¶å…³é”®è¯
        research_keywords = [
            "ç ”ç©¶", "è°ƒæŸ¥", "åˆ†æ", "æœç´¢", "æŸ¥æ‰¾", "äº†è§£",
            "research", "investigate", "analyze", "search", "find", "study"
        ]
        
        query_lower = query.lower()
        # å¦‚æœåŒ…å«å®æ—¶ä¿¡æ¯å…³é”®è¯æˆ–ç ”ç©¶å…³é”®è¯ï¼Œéƒ½åº”è¯¥æ‰§è¡Œæœç´¢
        return any(keyword in query_lower for keyword in realtime_keywords + research_keywords)

    async def _perform_research(self, query: str) -> Dict[str, Any]:
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        try:
            logger.info(f"å¼€å§‹ç ”ç©¶: {query}")
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = query.lower().strip()
            if cache_key in self.search_cache:
                logger.info("ä½¿ç”¨ç¼“å­˜çš„æœç´¢ç»“æœ")
                return self.search_cache[cache_key]
            
            # æ‰§è¡Œæœç´¢
            if self.use_searxng:
                search_results = await self._searxng_search(query)
            else:
                search_results = await self._brave_search(query)
            
            # åˆ†ææœç´¢ç»“æœ
            analyzed_results = await self._analyze_search_results(search_results)
            
            # ç¼“å­˜ç»“æœ
            research_result = {
                "query": query,
                "search_results": search_results,
                "analysis": analyzed_results,
                "timestamp": datetime.now().isoformat(),
                "sources_count": len(search_results.get("web", {}).get("results", []))
            }
            
            self.search_cache[cache_key] = research_result
            self.research_history.append(research_result)
            
            # æ›´æ–°æŒ‡æ ‡
            self.research_metrics["searches_performed"] += 1
            self.research_metrics["sources_analyzed"] += research_result["sources_count"]
            
            logger.info(f"ç ”ç©¶å®Œæˆï¼Œåˆ†æäº† {research_result['sources_count']} ä¸ªæ¥æº")
            return research_result
            
        except Exception as e:
            logger.error(f"ç ”ç©¶æ‰§è¡Œé”™è¯¯: {e}")
            return {"error": str(e), "query": query}

    async def _searxng_search(self, query: str, count: int = 10) -> Dict[str, Any]:
        """ä½¿ç”¨SearXNGè¿›è¡Œæœç´¢"""
        try:
            params = {
                "q": query,
                "format": "json",
                "categories": "general",
                "engines": "google,bing,duckduckgo",
                "safesearch": "1",
                "time_range": "week"
            }
            
            headers = {
                "Accept": "application/json",
                "User-Agent": "AutoGen-Research-Agent/1.0"
            }
            
            if self.search_engine_api_key:
                headers["Authorization"] = f"Bearer {self.search_engine_api_key}"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    self.search_endpoint,
                    headers=headers,
                    params=params,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return self._convert_searxng_to_standard_format(data, query)
                    else:
                        logger.error(f"SearXNGæœç´¢é”™è¯¯: {response.status}")
                        return self._get_mock_search_results(query)
                        
        except Exception as e:
            logger.error(f"SearXNGæœç´¢è¯·æ±‚é”™è¯¯: {e}")
            return self._get_mock_search_results(query)
    
    def _convert_searxng_to_standard_format(self, searxng_data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """å°†SearXNGç»“æœè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
        try:
            results = []
            for result in searxng_data.get("results", [])[:10]:  # é™åˆ¶å‰10ä¸ªç»“æœ
                results.append({
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "description": result.get("content", ""),
                    "date_published": result.get("publishedDate", datetime.now().isoformat()),
                    "engine": result.get("engine", "unknown")
                })
            
            return {
                "web": {
                    "results": results
                },
                "query": query,
                "type": "searxng_results",
                "engines_used": list(set([r.get("engine", "unknown") for r in searxng_data.get("results", [])]))
            }
        except Exception as e:
            logger.error(f"SearXNGç»“æœè½¬æ¢é”™è¯¯: {e}")
            return self._get_mock_search_results(query)
    
    async def _brave_search(self, query: str, count: int = 10) -> Dict[str, Any]:
        """ä½¿ç”¨Brave Search APIè¿›è¡Œæœç´¢"""
        if not self.brave_api_key:
            logger.warning("æœªé…ç½®Brave APIå¯†é’¥ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
            return self._get_mock_search_results(query)
        
        try:
            headers = {
                "Accept": "application/json",
                "Accept-Encoding": "gzip",
                "X-Subscription-Token": self.brave_api_key
            }
            
            params = {
                "q": query,
                "count": count,
                "search_lang": "zh",
                "country": "CN",
                "safesearch": "moderate",
                "freshness": "pw"  # è¿‡å»ä¸€å‘¨
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    self.search_endpoint,
                    headers=headers,
                    params=params,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        logger.error(f"Brave Search APIé”™è¯¯: {response.status}")
                        return self._get_mock_search_results(query)
                        
        except Exception as e:
            logger.error(f"Brave Searchè¯·æ±‚é”™è¯¯: {e}")
            return self._get_mock_search_results(query)

    def _get_mock_search_results(self, query: str) -> Dict[str, Any]:
        """è·å–æ¨¡æ‹Ÿæœç´¢ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        return {
            "web": {
                "results": [
                    {
                        "title": f"å…³äº {query} çš„ç ”ç©¶æŠ¥å‘Š",
                        "url": "https://example.com/research1",
                        "description": f"è¿™æ˜¯ä¸€ä»½å…³äº {query} çš„è¯¦ç»†ç ”ç©¶æŠ¥å‘Šï¼ŒåŒ…å«æœ€æ–°çš„å‘å±•è¶‹åŠ¿å’Œåˆ†æã€‚",
                        "date_published": datetime.now().isoformat()
                    },
                    {
                        "title": f"{query} æœ€æ–°å‘å±•åŠ¨æ€",
                        "url": "https://example.com/news1", 
                        "description": f"æœ€æ–°çš„ {query} å‘å±•åŠ¨æ€å’Œè¡Œä¸šåˆ†æï¼Œæä¾›ä¸“ä¸šè§è§£ã€‚",
                        "date_published": datetime.now().isoformat()
                    }
                ]
            },
            "query": query,
            "type": "mock_results"
        }

    async def _analyze_search_results(self, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææœç´¢ç»“æœ"""
        try:
            web_results = search_results.get("web", {}).get("results", [])
            
            if not web_results:
                return {"error": "æ²¡æœ‰æ‰¾åˆ°æœç´¢ç»“æœ"}
            
            # ä½¿ç”¨è®¤çŸ¥å·¥å…·åˆ†æ
            analysis = {
                "sources_analyzed": len(web_results),
                "key_topics": [],
                "credibility_assessment": {},
                "summary": "",
                "recommendations": []
            }
            
            # æå–å…³é”®ä¸»é¢˜
            all_text = " ".join([
                result.get("title", "") + " " + result.get("description", "")
                for result in web_results
            ])
            
            if hasattr(self, 'cognitive_tools') and all_text:
                cognitive_analysis = self.cognitive_tools.analyze_text(
                    all_text,
                    level=self.cognitive_level
                )
                
                analysis.update({
                    "cognitive_analysis": cognitive_analysis,
                    "key_topics": cognitive_analysis.get("key_concepts", []),
                    "summary": cognitive_analysis.get("summary", "")
                })
            
            # è¯„ä¼°æ¥æºå¯ä¿¡åº¦
            for i, result in enumerate(web_results):
                url = result.get("url", "")
                domain = url.split("//")[-1].split("/")[0] if "//" in url else url
                
                # ç®€å•çš„å¯ä¿¡åº¦è¯„ä¼°
                credibility_score = self._assess_source_credibility(domain)
                analysis["credibility_assessment"][domain] = credibility_score
            
            return analysis
            
        except Exception as e:
            logger.error(f"æœç´¢ç»“æœåˆ†æé”™è¯¯: {e}")
            return {"error": str(e)}

    def _assess_source_credibility(self, domain: str) -> Dict[str, Any]:
        """è¯„ä¼°æ¥æºå¯ä¿¡åº¦"""
        # ç®€åŒ–çš„å¯ä¿¡åº¦è¯„ä¼°
        trusted_domains = [
            "wikipedia.org", "nature.com", "science.org", "ieee.org",
            "arxiv.org", "pubmed.ncbi.nlm.nih.gov", "scholar.google.com"
        ]
        
        news_domains = [
            "bbc.com", "reuters.com", "ap.org", "cnn.com",
            "xinhuanet.com", "people.com.cn"
        ]
        
        if any(trusted in domain for trusted in trusted_domains):
            return {"score": 0.9, "type": "academic/reference", "reliability": "high"}
        elif any(news in domain for news in news_domains):
            return {"score": 0.7, "type": "news", "reliability": "medium-high"}
        else:
            return {"score": 0.5, "type": "general", "reliability": "medium"}

    async def _generate_research_report(self, query: str, research_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
        try:
            self.research_metrics["reports_generated"] += 1
            
            if "error" in research_result:
                return f"ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {research_result['error']}"
            
            # æ„å»ºæŠ¥å‘Š
            report = f"""# ç ”ç©¶æŠ¥å‘Š: {query}

## ğŸ“Š ç ”ç©¶æ¦‚è§ˆ
- **ç ”ç©¶æ—¶é—´**: {research_result.get('timestamp', 'æœªçŸ¥')}
- **åˆ†ææ¥æº**: {research_result.get('sources_count', 0)} ä¸ª
- **ç ”ç©¶ç±»å‹**: ç½‘ç»œæœç´¢ + è®¤çŸ¥åˆ†æ

## ğŸ” ä¸»è¦å‘ç°
"""
            
            # æ·»åŠ åˆ†æç»“æœ
            analysis = research_result.get("analysis", {})
            
            if "key_topics" in analysis and analysis["key_topics"]:
                report += "\n### å…³é”®ä¸»é¢˜\n"
                for topic in analysis["key_topics"][:5]:  # é™åˆ¶å‰5ä¸ª
                    report += f"- {topic}\n"
            
            if "summary" in analysis and analysis["summary"]:
                report += f"\n### ç ”ç©¶æ‘˜è¦\n{analysis['summary']}\n"
            
            # æ·»åŠ æ¥æºä¿¡æ¯
            search_results = research_result.get("search_results", {})
            web_results = search_results.get("web", {}).get("results", [])
            
            if web_results:
                report += "\n## ğŸ“š ä¿¡æ¯æ¥æº\n"
                for i, result in enumerate(web_results[:5], 1):  # é™åˆ¶å‰5ä¸ªæ¥æº
                    title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
                    url = result.get("url", "")
                    description = result.get("description", "")
                    
                    report += f"\n{i}. **{title}**\n"
                    report += f"   - é“¾æ¥: {url}\n"
                    if description:
                        report += f"   - æè¿°: {description[:200]}...\n"
            
            # æ·»åŠ å¯ä¿¡åº¦è¯„ä¼°
            credibility = analysis.get("credibility_assessment", {})
            if credibility:
                report += "\n## ğŸ”’ æ¥æºå¯ä¿¡åº¦è¯„ä¼°\n"
                for domain, assessment in credibility.items():
                    score = assessment.get("score", 0)
                    reliability = assessment.get("reliability", "æœªçŸ¥")
                    report += f"- {domain}: {score:.1f}/1.0 ({reliability})\n"
            
            # æ·»åŠ å»ºè®®
            report += "\n## ğŸ’¡ ç ”ç©¶å»ºè®®\n"
            report += "- å»ºè®®è¿›ä¸€æ­¥æŸ¥é˜…å­¦æœ¯æ–‡çŒ®ä»¥è·å¾—æ›´æ·±å…¥çš„è§è§£\n"
            report += "- å¯ä»¥å…³æ³¨æœ€æ–°çš„è¡Œä¸šæŠ¥å‘Šå’Œæ”¿ç­–æ–‡ä»¶\n"
            report += "- å»ºè®®äº¤å‰éªŒè¯å¤šä¸ªæ¥æºçš„ä¿¡æ¯\n"
            
            return report
            
        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆé”™è¯¯: {e}")
            return f"ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

    def get_research_metrics(self) -> Dict[str, Any]:
        """è·å–ç ”ç©¶æŒ‡æ ‡"""
        base_metrics = self.get_performance_metrics()
        return {
            **base_metrics,
            **self.research_metrics,
            "research_history_count": len(self.research_history),
            "cache_size": len(self.search_cache)
        }

    async def clear_research_cache(self):
        """æ¸…ç†ç ”ç©¶ç¼“å­˜"""
        self.search_cache.clear()
        logger.info("ç ”ç©¶ç¼“å­˜å·²æ¸…ç†")

    async def export_research_history(self, filepath: str):
        """å¯¼å‡ºç ”ç©¶å†å²"""
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.research_history, f, ensure_ascii=False, indent=2)
            logger.info(f"ç ”ç©¶å†å²å·²å¯¼å‡ºåˆ°: {filepath}")
        except Exception as e:
            logger.error(f"å¯¼å‡ºç ”ç©¶å†å²é”™è¯¯: {e}")


# å·¥å‚å‡½æ•°
async def create_research_agent(
    name: str = "ç ”ç©¶ä»£ç†",
    api_key: str = None,
    base_url: str = None,
    brave_api_key: str = None,
    model: str = "gpt-4o",
    **kwargs
) -> EnhancedResearchAgent:
    """åˆ›å»ºç ”ç©¶ä»£ç†çš„å·¥å‚å‡½æ•°"""
    
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("éœ€è¦æä¾›OpenAI APIå¯†é’¥")
    
    # åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯ - æ”¯æŒ Gemini
    if base_url and "gemini" in model.lower():
        # ä½¿ç”¨ Gemini å®¢æˆ·ç«¯
        model_client = create_gemini_client(
            model=model,
            api_key=api_key,
            base_url=base_url
        )
    else:
        # ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯
        client_kwargs = {
            "model": model,
            "api_key": api_key
        }
        
        if base_url:
            client_kwargs["base_url"] = base_url
            # ä¸ºéæ ‡å‡†æ¨¡å‹æä¾›model_info
            try:
                from autogen_ext.models.openai import ModelInfo
                client_kwargs["model_info"] = ModelInfo(
                    vision=True,
                    function_calling=True,
                    json_output=True
                )
            except ImportError:
                # å¦‚æœæ— æ³•å¯¼å…¥ModelInfoï¼Œä½¿ç”¨ç®€å•çš„å­—å…¸
                client_kwargs["model_info"] = {
                    "vision": True,
                    "function_calling": True,
                    "json_output": True
                }
        
        model_client = OpenAIChatCompletionClient(**client_kwargs)
    
    # åˆ›å»ºç ”ç©¶ä»£ç†
    agent = EnhancedResearchAgent(
        name=name,
        model_client=model_client,
        brave_api_key=brave_api_key,
        **kwargs
    )
    
    return agent


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    async def main():
        try:
            # åˆ›å»ºç ”ç©¶ä»£ç†
            research_agent = await create_research_agent(
                name="AIç ”ç©¶ä¸“å®¶",
                brave_api_key=os.getenv("BRAVE_API_KEY")
            )
            
            # æ¨¡æ‹Ÿç ”ç©¶ä»»åŠ¡
            from autogen_agentchat.messages import TextMessage
            
            test_message = TextMessage(
                content="è¯·ç ”ç©¶äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„æœ€æ–°åº”ç”¨",
                source="user"
            )
            
            # æ‰§è¡Œç ”ç©¶
            result = await research_agent.on_messages([test_message], None)
            print(f"ç ”ç©¶ç»“æœ:\n{result.content}")
            
            # æ˜¾ç¤ºæŒ‡æ ‡
            metrics = research_agent.get_research_metrics()
            print(f"\nç ”ç©¶æŒ‡æ ‡: {json.dumps(metrics, indent=2, ensure_ascii=False)}")
            
            # å…³é—­å®¢æˆ·ç«¯
            await research_agent.model_client.close()
            
        except Exception as e:
            logger.error(f"ç¤ºä¾‹è¿è¡Œé”™è¯¯: {e}")
    
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())
