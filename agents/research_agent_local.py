#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°ç ”ç©¶ä»£ç† - ä½¿ç”¨SearXNGæœç´¢å¼•æ“
==================================

ä½¿ç”¨æœ¬åœ°SearXNGæœç´¢å¼•æ“çš„ç ”ç©¶ä»£ç†
"""

import asyncio
import aiohttp
import json
import logging
import os
from typing import Dict, Any, List, Optional
from datetime import datetime
from urllib.parse import quote

logger = logging.getLogger(__name__)

class LocalResearchAgent:
    """ä½¿ç”¨æœ¬åœ°SearXNGæœç´¢å¼•æ“çš„ç ”ç©¶ä»£ç†"""
    
    def __init__(self, name: str = "local_research_agent"):
        self.name = name
        self.search_base_url = os.getenv("SEARCH_ENGINE_BASE_URL", "http://127.0.0.1:8081/search")
        self.ai_model_url = os.getenv("OPENAI_BASE_URL", "http://84.8.145.89:8000")
        self.session = None
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def _call_ai_model(self, prompt: str) -> str:
        """è°ƒç”¨Gemini Balanceä»£ç†æœåŠ¡å™¨"""
        try:
            # ä½¿ç”¨GeminiåŸç”ŸAPIæ ¼å¼
            ai_url = f"{self.ai_model_url}/v1beta/models/{os.getenv('OPENAI_MODEL', 'gemini-2.5-flash')}:generateContent"
            headers = {
                "Content-Type": "application/json",
                "x-goog-api-key": os.getenv('OPENAI_API_KEY', 'Hjd-961207hjd')
            }
            
            # Gemini Balanceä»£ç†æ‰€éœ€çš„æ ¼å¼
            payload = {
                "contents": [{
                    "role": "user",
                    "parts": [{
                        "text": prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": 0.7,
                    "maxOutputTokens": 1000,
                    "topP": 0.8,
                    "topK": 10
                }
            }
            
            logger.info(f"è°ƒç”¨Gemini API: {ai_url}")
            async with self.session.post(ai_url, json=payload, headers=headers) as response:
                if response.status == 200:
                    result = await response.json()
                    # Gemini APIå“åº”æ ¼å¼
                    if 'candidates' in result and len(result['candidates']) > 0:
                        content = result['candidates'][0]['content']['parts'][0]['text']
                        logger.info("AIåˆ†ææˆåŠŸ")
                        return content
                    else:
                        logger.error(f"Geminiå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                        return "æ— æ³•è§£æAIå“åº”ç»“æœ"
                else:
                    response_text = await response.text()
                    logger.error(f"Gemini APIè°ƒç”¨å¤±è´¥: {response.status}, å“åº”: {response_text}")
                    return f"æ— æ³•è·å–AIåˆ†æç»“æœ (çŠ¶æ€ç : {response.status})"
        except Exception as e:
            logger.error(f"Gemini APIè°ƒç”¨é”™è¯¯: {e}")
            return f"åˆ†æé”™è¯¯: {str(e)}"
    
    async def _translate_to_chinese(self, text: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹å°†æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡"""
        if not text or len(text.strip()) == 0:
            return text
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ä¸­æ–‡
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        if chinese_chars / len(text) > 0.5:  # å¦‚æœä¸­æ–‡å­—ç¬¦è¶…è¿‡50%ï¼Œè®¤ä¸ºå·²ç»æ˜¯ä¸­æ–‡
            return text
        
        translate_prompt = f"""è¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œä¿æŒåŸæ„å’Œä¸“ä¸šæ€§ï¼š

{text}

è¯·åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚"""
        
        try:
            translated = await self._call_ai_model(translate_prompt)
            return translated.strip()
        except Exception as e:
            logger.error(f"ç¿»è¯‘é”™è¯¯: {e}")
            return text  # ç¿»è¯‘å¤±è´¥æ—¶è¿”å›åŸæ–‡
    
    async def search_web(self, query: str, max_results: int = 10) -> Dict[str, Any]:
        """ä½¿ç”¨SearXNGè¿›è¡Œç½‘ç»œæœç´¢"""
        try:
            if not self.session:
                self.session = aiohttp.ClientSession()
            
            # æ„å»ºæœç´¢URL
            encoded_query = quote(query)
            search_url = f"{self.search_base_url}?q={encoded_query}&format=json"
            
            logger.info(f"æ­£åœ¨æœç´¢: {query}")
            logger.info(f"æœç´¢URL: {search_url}")
            
            async with self.session.get(search_url) as response:
                if response.status == 200:
                    search_data = await response.json()
                    
                    # å¤„ç†SearXNGå“åº”æ ¼å¼
                    results = []
                    if 'results' in search_data:
                        for idx, item in enumerate(search_data['results'][:max_results]):
                            results.append({
                                "title": item.get("title", ""),
                                "url": item.get("url", ""),
                                "description": item.get("content", ""),
                                "relevance_score": round(1.0 - (idx * 0.1), 2),
                                "source": item.get("engine", "SearXNG")
                            })
                    
                    summary_text = ""
                    for result in results:
                        summary_text += result.get("description", "") + " "
                    
                    return {
                        "query": query,
                        "total_results": len(results),
                        "web_results": results,
                        "search_timestamp": str(datetime.now()),
                        "search_engine": "SearXNG (local)",
                        "status": "success",
                        "summary": summary_text
                    }
                else:
                    return {
                        "query": query,
                        "total_results": 0,
                        "web_results": [],
                        "error": f"æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}",
                        "search_timestamp": str(datetime.now()),
                        "status": "error"
                    }
                    
        except aiohttp.ClientConnectorError:
            return {
                "query": query,
                "total_results": 0,
                "web_results": [],
                "error": "æ— æ³•è¿æ¥åˆ°æœ¬åœ°SearXNGæœåŠ¡ï¼Œè¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨",
                "search_timestamp": str(datetime.now()),
                "status": "error"
            }
        except Exception as e:
            logger.error(f"æœç´¢é”™è¯¯: {e}")
            return {
                "query": query,
                "total_results": 0,
                "web_results": [],
                "error": str(e),
                "search_timestamp": str(datetime.now()),
                "status": "error"
            }
    
    async def analyze_search_results(self, search_results: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨AIæ¨¡å‹åˆ†ææœç´¢ç»“æœ"""
        if search_results.get("status") != "success":
            return search_results
            
        results = search_results["web_results"]
        query = search_results.get("query", "")
        
        # æ„å»ºåˆ†ææç¤ºè¯
        content_text = "\n".join([
            f"æ ‡é¢˜: {result.get('title', '')}"
            f"æè¿°: {result.get('description', '')}"
            f"æ¥æº: {result.get('url', '')}"
            for result in results[:5]  # åªåˆ†æå‰5ä¸ªç»“æœ
        ])
        
        analysis_prompt = f"""ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åˆ†æå¸ˆï¼Œè¯·å¯¹ä»¥ä¸‹æœç´¢ç»“æœè¿›è¡Œæ·±åº¦åˆ†æï¼š

æŸ¥è¯¢ä¸»é¢˜: {query}

æœç´¢ç»“æœ:
{content_text}

è¯·æä¾›ï¼š
1. å…³é”®æ¦‚å¿µå’Œä¸»é¢˜ï¼ˆä¸­æ–‡ï¼‰
2. ä¸»è¦å‘ç°å’Œè§è§£ï¼ˆä¸­æ–‡ï¼‰
3. ä¿¡æ¯å¯ä¿¡åº¦è¯„ä¼°
4. å»ºè®®çš„åç»­è¡ŒåŠ¨ï¼ˆä¸­æ–‡ï¼‰

è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œä¿æŒä¸“ä¸šå’Œå®¢è§‚ã€‚"""
        
        try:
            # è°ƒç”¨AIæ¨¡å‹è¿›è¡Œåˆ†æ
            ai_analysis = await self._call_ai_model(analysis_prompt)
            
            return {
                "analysis_metadata": {
                    "query": query,
                    "analyzed_at": str(datetime.now()),
                    "agent": self.name,
                    "ai_model": "gemini-2.5-flash"
                },
                "ai_analysis": ai_analysis,
                "content_summary": f"ä½¿ç”¨AIæ¨¡å‹å¯¹{len(results)}ä¸ªæœç´¢ç»“æœè¿›è¡Œäº†æ·±åº¦åˆ†æ",
                "total_sources": len(results)
            }
        except Exception as e:
            logger.error(f"AIåˆ†æé”™è¯¯: {e}")
            # å›é€€åˆ°åŸºæœ¬åˆ†æ
            return {
                "analysis_metadata": {
                    "query": query,
                    "analyzed_at": str(datetime.now()),
                    "agent": self.name
                },
                "content_summary": f"ä»{len(results)}ä¸ªæœç´¢ç»“æœä¸­æå–äº†åŸºæœ¬ä¿¡æ¯",
                "total_sources": len(results),
                "error": f"AIåˆ†æå¤±è´¥: {str(e)}"
            }
    
    async def generate_research_report(self, search_results: Dict[str, Any], 
                                     analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºæœç´¢ç»“æœå’ŒAIåˆ†æç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
        query = search_results.get("query", "ç ”ç©¶ä¸»é¢˜")
        results = search_results.get("web_results", [])
        ai_analysis = analysis.get("ai_analysis", "æœªè¿›è¡ŒAIåˆ†æ")
        
        if not results:
            return {
                "report_metadata": {
                    "title": f"ç ”ç©¶æŠ¥å‘Š: {query}",
                    "generated_at": str(datetime.now()),
                    "agent": self.name,
                    "status": "no_results"
                },
                "executive_summary": f"æœªèƒ½æ‰¾åˆ°å…³äº{query}çš„è¶³å¤Ÿä¿¡æ¯",
                "key_findings": [],
                "recommendations": ["å°è¯•ä¸åŒçš„æœç´¢å…³é”®è¯", "æ£€æŸ¥SearXNGæœåŠ¡çŠ¶æ€"]
            }
        
        # æ„å»ºè¯¦ç»†çš„ä¸­æ–‡ç ”ç©¶æŠ¥å‘Š
        detailed_sources = []
        key_findings = []
        
        for idx, result in enumerate(results[:8]):  # å¢åŠ åˆ°8ä¸ªç»“æœ
            # æå–æ¯æ¡æ¶ˆæ¯çš„è¯¦ç»†å†…å®¹
            original_title = result.get("title", "æ— æ ‡é¢˜")
            original_description = result.get("description", "")
            url = result.get("url", "")
            source = result.get("source", "æœªçŸ¥")
            
            # ä½¿ç”¨AIç¿»è¯‘æ ‡é¢˜å’Œæè¿°ä¸ºä¸­æ–‡
            title = await self._translate_to_chinese(original_title)
            description = await self._translate_to_chinese(original_description)
            
            # ç”Ÿæˆå…³é”®è¦ç‚¹
            key_points = []
            if description:
                # æå–å‰3ä¸ªå…³é”®å¥å­
                sentences = description.split('ã€‚')  # ä½¿ç”¨ä¸­æ–‡å¥å·
                if len(sentences) <= 1:
                    sentences = description.split('.')  # å¦‚æœæ²¡æœ‰ä¸­æ–‡å¥å·ï¼Œä½¿ç”¨è‹±æ–‡å¥å·
                key_points = [s.strip() + ('ã€‚' if not s.strip().endswith('ã€‚') else '') 
                             for s in sentences[:3] if s.strip()]
            
            # ç”Ÿæˆè¯¦ç»†å‘ç°
            key_findings.append({
                "title": title,
                "url": url,
                "key_points": key_points,
                "summary": description[:200] + "..." if len(description) > 200 else description,
                "relevance": result.get("relevance_score", 0.0),
                "source": source,
                "timestamp": str(datetime.now())
            })
            
            # æ·»åŠ åˆ°è¯¦ç»†æ¥æº
            detailed_sources.append({
                "id": idx + 1,
                "title": title,
                "url": url,
                "description": description,
                "source": source,
                "relevance": result.get("relevance_score", 0.0),
                "domain": result.get("domain", "unknown")
            })
        
        # ç”ŸæˆåŒ…å«AIåˆ†æçš„è¯¦ç»†æŠ¥å‘Š
        return {
            "report_metadata": {
                "title": f"ğŸ” ç ”ç©¶æŠ¥å‘Šï¼š{query}",
                "generated_at": str(datetime.now()),
                "agent": self.name,
                "status": "success",
                "search_engine": "SearXNG (æœ¬åœ°)",
                "ai_model": "Gemini-2.5-flash",
                "total_sources": len(results),
                "confidence_score": round(sum([r.get("relevance_score", 0.0) for r in results]) / len(results) if results else 0.75, 2)
            },
            "executive_summary": f"åŸºäºæœ¬åœ°SearXNGæœç´¢å¼•æ“å’ŒGemini-2.5-flash AIæ¨¡å‹ï¼ŒæˆåŠŸæ‰¾åˆ°{len(detailed_sources)}ä¸ªé«˜è´¨é‡ä¿¡æ¯æºï¼Œæ¶µç›–{query}çš„å„ä¸ªæ–¹é¢",
            "ai_analysis": ai_analysis,
            "key_findings": key_findings,
            "sources": detailed_sources,
            "detailed_analysis": {
                "search_query": query,
                "total_results": len(results),
                "unique_sources": len(set([r.get("source", "unknown") for r in results])),
                "average_relevance": round(sum([r.get("relevance_score", 0.0) for r in results]) / len(results), 2),
                "top_domains": list(set([r.get("domain", "unknown") for r in results]))[:5]
            },
            "recommendations": [
                "ä¼˜å…ˆæŸ¥çœ‹é«˜ç›¸å…³æ€§(>0.7)çš„ä¿¡æ¯æº",
                "éªŒè¯å…³é”®ä¿¡æ¯çš„å¤šä¸ªæ¥æºä»¥ç¡®ä¿å‡†ç¡®æ€§",
                "å…³æ³¨æƒå¨åŸŸå(.edu, .gov, .org)çš„ä¿¡æ¯",
                "å®šæœŸé‡æ–°æœç´¢ä»¥è·å–æœ€æ–°å‘å±•",
                "ç»“åˆå¤šä¸ªä¿¡æ¯æºå½¢æˆå…¨é¢è§‚ç‚¹"
            ],
            "raw_data": {
                "total_searched": len(results),
                "processed_results": len(detailed_sources),
                "search_timestamp": str(datetime.now()),
                "query_processed": query
            }
        }
    
    async def search_and_analyze(self, query: str) -> Dict[str, Any]:
        """å®Œæ•´çš„æœç´¢å’Œåˆ†ææµç¨‹"""
        try:
            # 1. æœç´¢
            search_results = await self.search_web(query)
            
            if search_results.get("status") == "error":
                return search_results
            
            # 2. åˆ†æ
            analysis = await self.analyze_search_results(search_results)
            
            # 3. ç”ŸæˆæŠ¥å‘Š
            report = await self.generate_research_report(search_results, analysis)
            
            return {
                "query": query,
                "search_results": search_results,
                "analysis": analysis,
                "research_report": report,
                "workflow_status": "success",
                "timestamp": str(datetime.now())
            }
            
        except Exception as e:
            logger.error(f"æœç´¢åˆ†ææµç¨‹é”™è¯¯: {e}")
            return {
                "query": query,
                "error": str(e),
                "workflow_status": "error",
                "timestamp": str(datetime.now())
            }
